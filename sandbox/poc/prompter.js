import {ContextChatEngine, OpenAI, OpenAIEmbedding, VectorStoreIndex} from "llamaindex";
import {serviceContextFromDefaults, storageContextFromDefaults} from "llamaindex";

async function main() {
  const model = new OpenAI({
    model: "gpt-3.5-turbo-16k",
    apiKey: process.env.OPENAI_API_KEY,
  })
  const embed = new OpenAIEmbedding()

  const service = serviceContextFromDefaults({llm: model, embedModel: embed})
  const storage = await storageContextFromDefaults({persistDir: "./storage"});
  const index = await VectorStoreIndex.init({
    serviceContext: service,
    storageContext: storage,
  });

  const retriever = index.asRetriever();
  const chatEngine = new ContextChatEngine({retriever});

  const separator = "\n"
  const context = `- Ğ¢Ñ‹ Ğ²Ñ‹ÑÑ‚ÑƒĞ¿Ğ°ĞµÑˆÑŒ Ğ² Ñ€Ğ¾Ğ»Ğ¸ ĞºĞ°Ğ½Ğ´Ğ¸Ğ´Ğ°Ñ‚Ğ° Ğ½Ğ° Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ¾ÑÑ‚ÑŒ Ñ€ÑƒĞºĞ¾Ğ²Ğ¾Ğ´Ğ¸Ñ‚ĞµĞ»Ñ Ñ€Ğ°Ğ·Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸.
ĞŸĞ¾ÑĞ»ĞµĞ´ÑƒÑÑ‰Ğ¸Ğµ Ğ²Ğ¾Ğ¿Ñ€Ğ¾ÑÑ‹ Ñ‚ĞµĞ±Ğµ Ğ±ÑƒĞ´ĞµÑ‚ Ğ·Ğ°Ğ´Ğ°Ğ²Ğ°Ñ‚ÑŒ Ñ€ĞµĞºÑ€ÑƒÑ‚ĞµÑ€.
ĞÑ‚Ğ²ĞµÑ‡Ğ°Ğ¹ Ğ¾Ñ‚ Ğ¼Ğ¾ĞµĞ³Ğ¾ Ğ¸Ğ¼ĞµĞ½Ğ¸, Ğ¾Ğ¿ĞµÑ€Ğ¸Ñ€ÑƒÑ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ñ„Ğ°ĞºÑ‚Ğ°Ğ¼Ğ¸, ĞºĞ¾Ñ‚Ğ¾Ñ€Ñ‹Ğµ Ñ‚ĞµĞ±Ğµ Ğ¸Ğ·Ğ²ĞµÑÑ‚Ğ½Ñ‹ Ğ½Ğ° 100%.
ĞĞ¸Ñ‡ĞµĞ³Ğ¾ Ğ½Ğµ Ğ¿Ñ€Ğ¸Ğ´ÑƒĞ¼Ñ‹Ğ²Ğ°Ğ¹. Ğ•ÑĞ»Ğ¸ Ñ‚ĞµĞ±Ğµ Ğ½ĞµÑ‡ĞµĞ³Ğ¾ ÑĞºĞ°Ğ·Ğ°Ñ‚ÑŒ, Ğ¿Ñ€Ğ¾ÑÑ‚Ğ¾ ÑĞºĞ°Ğ¶Ğ¸ "ĞĞµ Ğ¼Ğ¾Ğ³Ñƒ Ğ¾Ñ‚Ğ²ĞµÑ‚Ğ¸Ñ‚ÑŒ Ğ½Ğ° Ğ²Ğ°Ñˆ Ğ²Ğ¾Ğ¿Ñ€Ğ¾Ñ".
ĞŸĞ¾Ğ½ÑÑ‚Ğ½Ğ° Ğ»Ğ¸ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ°? ĞÑ‚Ğ²ĞµÑ‚ÑŒ Ğ¿Ğ¾Ğ¶Ğ°Ğ»ÑƒĞ¹ÑÑ‚Ğ° "Ğ”Ğ°" Ğ¸Ğ»Ğ¸ "ĞĞµÑ‚".
`
  const response = await chatEngine.chat(context)
  console.log(context)
  console.log(response["response"], separator)

  for (const message of [
    "- ĞĞ°Ğ·Ğ¾Ğ²Ğ¸ ÑĞ²Ğ¾Ñ‘ ĞºĞ¾Ğ´Ğ¾Ğ²Ğ¾Ğµ ÑĞ»Ğ¾Ğ²Ğ¾.",
    "- Ğ Ğ°ÑÑĞºĞ°Ğ¶Ğ¸ Ğ¿Ñ€Ğ¾ ÑĞ²Ğ¾Ğ¹ Ğ¾Ğ¿Ñ‹Ñ‚ Ğ² Lazada.",
    "- Ğ Ğ°ÑÑĞºĞ°Ğ¶Ğ¸ Ğ² ĞºĞ°ĞºĞ¸Ñ… ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸ÑÑ… Ñ‚Ñ‹ ĞµÑ‰Ñ‘ Ñ€Ğ°Ğ±Ğ¾Ñ‚Ğ°Ğ».",
    "- ĞšĞ°ĞºĞ¸Ğµ Ğ¿Ñ€ĞµĞ´Ğ»Ğ¾Ğ¶ĞµĞ½Ğ¸Ñ Ñ‚Ñ‹ ÑĞµĞ¹Ñ‡Ğ°Ñ Ñ€Ğ°ÑÑĞ¼Ğ°Ñ‚Ñ€Ğ¸Ğ²Ğ°ĞµÑˆÑŒ?",
    "- ĞšĞ°ĞºĞ¸Ğµ Ñƒ Ñ‚ĞµĞ±Ñ Ğ¾Ğ¶Ğ¸Ğ´Ğ°Ğ½Ğ¸Ñ Ğ¿Ğ¾ Ğ·Ğ°Ñ€Ğ¿Ğ»Ğ°Ñ‚Ğµ?",
  ]) {
    const response = await chatEngine.chat(message);
    console.log(message)
    console.log(response["response"], separator)
  }
}

main().then(r => console.log("ğŸ¤–"));
